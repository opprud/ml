{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "walkthrough MOJ.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/opprud/ml/blob/master/walkthrough_MOJ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWZClAWxFqA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv1D, Dense, Dropout, BatchNormalization, MaxPooling1D, Activation, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import preprocess\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMFIOd4GGNeA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = 'data.zip'\n",
        "\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "\n",
        "zip_ref.extractall('/data')\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRUPbQI3FqBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training parametera\n",
        "batch_size = 128\n",
        "epochs = 20\n",
        "num_classes = 10\n",
        "length = 2048\n",
        "BatchNorm = True # Whether to batch normalize\n",
        "number = 1000 # Number of samples per class\n",
        "normal = True # Is it standardized?\n",
        "rate = [0.7,0.2,0.1] # Test set verification set division ratio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXs8_5mJFqBF",
        "colab_type": "text"
      },
      "source": [
        "## import train/test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKwFyXdPFqBG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#path = r'data\\0HP'\n",
        "path = r'./data/0HP'\n",
        "\n",
        "x_train, y_train, x_valid, y_valid, x_test, y_test = preprocess.prepro(d_path=path,length=length,\n",
        "                                                                  number=number,\n",
        "                                                                  normal=normal,\n",
        "                                                                  rate=rate,\n",
        "                                                                  enc=True, enc_step=28)\n",
        "# When you input the convolution, you need to modify it to increase the number of channels\n",
        "x_train, x_valid, x_test = x_train[:,:,np.newaxis], x_valid[:,:,np.newaxis], x_test[:,:,np.newaxis]\n",
        "# Enter the dimensions of the data\n",
        "input_shape =x_train.shape[1:]\n",
        "\n",
        "print('Training sample dimension:', x_train.shape)\n",
        "print(x_train.shape[0], 'Number of training samples')\n",
        "print('Verify the dimensions of the sample', x_valid.shape)\n",
        "print(x_valid.shape[0], 'Verify the number of samples')\n",
        "print('Test sample dimensions', x_test.shape)\n",
        "print(x_test.shape[0], 'Number of test samples')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCdVHmFtFqBL",
        "colab_type": "text"
      },
      "source": [
        "## Asses class imbalance\n",
        "Count the number of occurences in the training dataset, to see wether we have class imbalance\n",
        "The numbers show we have an equal number of active outputs ('1') in each row... so all is OK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sah5AW7bFqBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj4wDvrLFqBP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(y_test)\n",
        "print(df.head())\n",
        "\n",
        "for i in range(10):\n",
        "    print(df[i].value_counts())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYA8rspwFqBS",
        "colab_type": "text"
      },
      "source": [
        "# Define model\n",
        "wdcnn layer definition, each layer contains\n",
        "* A 1D conviolution filter\n",
        "* Optional batch normalisation\n",
        "* A relu activation\n",
        "* A MAX pool for dimesionality reduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBoL4gdXFqBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the convolution layer\n",
        "def wdcnn(filters, kernerl_size, strides, conv_padding, pool_padding,  pool_size, BatchNormal):\n",
        "    \"\"\"wdcnn Layer neuron\n",
        "\n",
        "    :param filters: Number of convolution kernels, integer\n",
        "    :param kernerl_size: Convolution kernel size, integer\n",
        "    :param strides: Step size, integer\n",
        "    :param conv_padding: 'same','valid'\n",
        "    :param pool_padding: 'same','valid'\n",
        "    :param pool_size: Pooled layer core size, integer\n",
        "    :param BatchNormal: Whether Batchnormal, Boolean\n",
        "    :return: model\n",
        "    \"\"\"\n",
        "    model.add(Conv1D(filters=filters, kernel_size=kernerl_size, strides=strides,\n",
        "                     padding=conv_padding, kernel_regularizer=l2(1e-4)))\n",
        "    if BatchNormal:\n",
        "        model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling1D(pool_size=pool_size, padding=pool_padding))\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2HKcDmeFqBU",
        "colab_type": "text"
      },
      "source": [
        "## Build a sequential deep model\n",
        "The model is composed of a number of layers, as proposed in the article.\n",
        "The model is build from 1Dconvolutional filters combined with BatchNormalisation to ensure the full dynamic range of the layers is utilised.\n",
        "The model is composed as follows\n",
        "* Five convolutional layers\n",
        "* A flattening layer before\n",
        "* Two fully connected layers\n",
        "\n",
        "**Relu** functions are used for activation, and in the last layer a **Softmax** activation, to allow classification into **num_classes** categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kj39yOEgFqBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiated sequential model\n",
        "model = Sequential()\n",
        "# Set up the input layer, the first layer of convolution. Because you want to specify input_shape, it is released separately.\n",
        "model.add(Conv1D(filters=16, kernel_size=64, strides=16, padding='same',kernel_regularizer=l2(1e-4), input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# second layer conv\n",
        "model = wdcnn(filters=32, kernerl_size=3, strides=1, conv_padding='same',\n",
        "              pool_padding='valid',  pool_size=2, BatchNormal=BatchNorm)\n",
        "# Third layer conv\n",
        "model = wdcnn(filters=64, kernerl_size=3, strides=1, conv_padding='same',\n",
        "              pool_padding='valid', pool_size=2, BatchNormal=BatchNorm)\n",
        "# Fourth layer conv\n",
        "model = wdcnn(filters=64, kernerl_size=3, strides=1, conv_padding='same',\n",
        "              pool_padding='valid', pool_size=2, BatchNormal=BatchNorm)\n",
        "# Fifth layer conv\n",
        "model = wdcnn(filters=64, kernerl_size=3, strides=1, conv_padding='valid',\n",
        "              pool_padding='valid', pool_size=2, BatchNormal=BatchNorm)\n",
        "# Flatten from convolution to full connection\n",
        "model.add(Flatten())\n",
        "\n",
        "# Flatten from convolution to full connection\n",
        "model.add(Dense(units=100, activation='relu', kernel_regularizer=l2(1e-4)))\n",
        "# Decrease the output layer\n",
        "model.add(Dense(units=num_classes, activation='softmax', kernel_regularizer=l2(1e-4)))\n",
        "\n",
        "\n",
        "# Compiling the model The evaluation function is similar to the loss function, \n",
        "# but the results of the evaluation function are not used in the training process.\n",
        "model.compile(optimizer='Adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "# TensorBoard Call to check the training situation\n",
        "tb_cb = TensorBoard(log_dir='logs')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY929Iq0FqBV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDbRgtM4FqBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxPZdgWgFqBZ",
        "colab_type": "text"
      },
      "source": [
        "## optional skip traning if models has been saved\n",
        "Using google colab and GPU for training is much more efficient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fg9rAycMFqBa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start model training \n",
        "model.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=epochs,\n",
        "          verbose=1, validation_data=(x_valid, y_valid), shuffle=True,\n",
        "          callbacks=[tb_cb])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFe_DeD4FqBc",
        "colab_type": "text"
      },
      "source": [
        "## Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0lHijg4FqBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'keras_wdcnn_2.h5'\n",
        "\n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "    \n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCyvdC8gFqBe",
        "colab_type": "text"
      },
      "source": [
        "## Load pretrained model instead"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rW98CmymFqBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls savedModels/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWQEqob6FqBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(\"./saved_models/keras_wdcnn_2.h5\")\n",
        "print(\"Loaded model from disk\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8Zn_7_FFqBi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate\n",
        "score = model.evaluate(x=x_train, y=y_train, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ7JEnVBFqBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate\n",
        "score = model.evaluate(x=x_test, y=y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzRgGm-XFqBm",
        "colab_type": "text"
      },
      "source": [
        "## Plot model layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZjA48WIFqBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_model(model=model, to_file='wdcnn.png', show_shapes=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I01fEEtcFqBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tgNovJsFqBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tensorboard --logdir='logs'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeOlmVrRFqBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}